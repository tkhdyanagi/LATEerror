---
title: "**LATEerror**"
author: "Takahide Yanagi (t.yanagi@r.hit-u.ac.jp)"
date: "January, 2018"
#documentclass: ltjarticle
#output: 
#  pdf_document:
#    latex_engine: lualatex
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignette explains how to use the package **LATEerror** in **R**.
The package contains the function `LATEerror` that implements the generalized method of moments (GMM) inference developed in Yanagi (2017).
The function estimates the local average treatment effect (LATE) with standard errors and 95\% confidence intervals when the binary treatment variable may contain a measurement error. 
It also contains the auxiliary functions `addindicators`, `criterion`, and `inference` that allow you to implement the GMM inference on your own.

Below we explain the setting, the parameters to be estimated, the GMM estimation, the practical implementation, the arguments and the values for `LATEerror`, and a simple example.
Here we do not explain identification for the LATE with the measurement error and theoretical properties of the GMM inference.
See Yanagi (2017) for details on the identification and theoretical properties.


## Setting

Let $\{(Y_i, D_i, Z_i, V_i)\}_{i=1}^n$ be a random sample.

* $Y_i \in \mathbb{R}$ is an outcome.

* $D_i \in \{0, 1\}$ is a binary treatment that may be mismeasured for the true treatment $D_i^* \in \{0, 1\}$.

* $Z_i \in \{0, 1\}$ is a binary instrument.

* $V_i \in \{1, 2, \dots, K \}$ is an exogenous variable such as a covariate, second instrument, or a repeated measure of the treatment where $K$ is some positive integer. 


## Parameters

The parameters of interest are the following $2K + 10$ parameters
\begin{align*}
  \theta_0 = (\beta^*, \Delta p^*, r, \tau_0^*, \tau_1^*, m_{00}, m_{01}, m_{10}, m_{11}, p_{01}^*, \dots, p_{0K}^*, p_{11}^*, \dots, p_{1K}^*)^\top.
\end{align*}
where

* $\beta^*$ is the LATE that is identical to the true instrumental variables estimator $[E(Y|Z=1) - E(Y|Z=0)]/[E(D^*|Z=1) - E(D^*|Z=0)]$.
* $\Delta p^* = E(D^*|Z=1) - E(D^*|Z=0)$ is the true first-stage regression.
* $r = E(Z)$ is the mean of $Z$.
* $\tau_z^* = E(Y|D^*=1, Z = z) - E(Y|D^*=0, Z = z)$ is the difference of the conditional means.
* $m_{dz} = \Pr(D \neq D^*|D^*=d, Z = z)$ is the misclassification probability.
* $p_{zv}^* = \Pr(D^*=d|Z = z, V = v)$ is the conditional true treatment probability.

$\beta^*$ and $\Delta p^*$ are the main parameters of interest.
The other parameters are nuisance parameters due to the measurement error for the treatment.

Yanagi (2017) shows identification of $\theta_0$ under identification conditions.

If we assume $m_{tz} = m_{t}$ so that the misclassification probability does not depend on $Z$ as in Assumption 4.4 (ii) of Yanagi (2017), the number of the parameters reduces to $2K+8$ such that
\begin{align*}
  \theta_0 = (\beta^*, \Delta p^*, r, \tau_0^*, \tau_1^*, m_{0}, m_{1}, p_{01}^*, \dots, p_{0K}^*, p_{11}^*, \dots, p_{1K}^*)^\top.
\end{align*}


## GMM estimation

`LATEerror` obtains the GMM estimate $\hat \theta$ for the parameter $\theta_0$ by minimizing the following GMM criterion with respect to $\theta$:
\begin{align*}
  \left(\frac{1}{n} \sum_{i=1}^n g(X_i, \theta) \right)^\top \hat \Lambda \left(\frac{1}{n} \sum_{i=1}^n g(X_i, \theta) \right)
\end{align*}
where $X_i = (Y_i, D_i, Z_i, V_i)^\top$ and $\hat \Lambda$ is a $(4 K + 3) \times (4 K + 3)$ weighting matrix and $g(X, \theta)$ contains the following $4K+3$ elements:
\begin{align*}
  & \beta^* - \frac{YZr^{-1} - Y(1-Z)(1-r)^{-1}}{\Delta p^*},\\
	& \Delta p^* - \left( \frac{DZr^{-1} -m_{01}}{1-m_{01}-m_{11}} - \frac{D(1-Z)(1-r)^{-1} -m_{00}}{1-m_{00}-m_{10}} \right),\\
	& r - Z,\\
	& \Big( m_{0z} + (1-m_{0z}-m_{1z})p_{zv_k}^* - D \Big) I_{zv_k},\\
	& \left( \tau_z^* + \frac{YD-(1-m_{1z})p^*_{z v_k}\tau_z^*}{m_{0z} + (1-m_{0z}-m_{1z})p^*_{zv_k}} - \frac{Y(1-D)+(1-m_{0z})(1-p^*_{zv_k})\tau_z^*}{1-(m_{0z}+(1-m_{0z}-m_{1z})p^*_{z v_k})}\right) I_{zv_k},
\end{align*}
where $I_{zv_k} = \mathbf{1}(Z=z, V=k)$ is the indicator.
The GMM estimator is based on the moment condition $E[g(X, \theta_0)] = 0$ implied by the identification of $\theta_0$.

The GMM estimator is asymptotically normally distributed, so that we can implement standard inferences for $\theta_0$ such as confidence intervals estimation and over-identification test (i.e. $J$ test).

We can obtain the optimal GMM estimate based on two-step estimation.
We first obtain a pilot GMM estimate by using the identity matrix as the weighting matrix.
We then obtain the optimal GMM estimate by an estimated optimal weighting matrix based on the pilot GMM estimate.
See Yanagi (2017) for details.

## Practical implementation

The implementation of the GMM estimation requires solving a nonlinear optimization.
Since the GMM criterion may have local minima, gradient-based local optimization such as quasi-Newton methods may work poorly since their performance severely depends on an initial value for the optimization.
To overcome the problem, `LATEerror` implements a global optimization procedure based on Differential Evolution (DE) through `DEoptim` by Mullen, Ardia, Gil, Windover, and Cline (2011).
DE is a metaheuristics that is not a gradient-based optimization, so that it can circumvent the problem due to local minima.

## Arguments of `LATEerror`

The usage for `LATEerror` is as follows.

    LATEerror(Y, D, Z, V, weight = NULL, optimal = FALSE, equal = TRUE, lower = NULL, upper = NULL, control = NULL)

The variables `Y`, `D`, `Z`, and `V` must be specified, but the others are optional.

* `Y`: A vector of the outcome variables

* `D`: A vector of the treatment variables

* `Z`: A vector of the instrumental variables

* `V`: A vector of the exogenous variables

* `weight`: A matrix for the weighting matrix in the pilot GMM estimation. 
If `weight` is not specified, the identity matrix is selected as the weighting matrix.

* `optimal`: Logical whether conducting the optimal GMM estimation.
If `optimal = TRUE`, the two-step optimal GMM estimation is implemented.
The default is FALSE.

* `equal`: Logical whether assuming that the misclassification probabilities do not depend on the instrument. 
When $V$ takes only two values, `equal = TRUE` must be specified.
The default is TRUE.

* `lower`: A vector of lower bounds of the parameters.
By default, the lower bounds of the LATE and $\tau_z^*$ are `min(Y) - max(Y)`, that of the true first-stage is 0.001, that of the misclassification probability is 0.001, and those of the other probabilities are 0.001.

* `upper`: A vector of upper bounds of the parameters.
By default, the upper bounds of the LATE and $\tau_z^*$ are `max(Y) - min(Y)`, that of the true first-stage is 0.999, that of the misclassification probability is 0.499, and those of the other probabilities are 0.999.

* `control`: A list for the control parameters for `DEoptim` function.
The same arguments for `control` in `DEoptim` can be specified.
See [DEoptim](https://cran.r-project.org/web/packages/DEoptim/index.html) for details.
Among the control parameters, `strategy`, `NP`, `itermax`, `CR`, `F`, `initialpop`, `c`, `reltol`, and `steptol` are important for the optimization.
The performance of the optimization may severely depends on these variables, so that several value for them should be tried.
By default, `LATEerror` sets `strategy = 2`, `NP = 10 * length(lower)`, `itermax = 10000`, `CR = 0.5`, `F = 0.8`, `initialpop = NULL`, `c = 0.05`, `reltol = 1e-12`, and `steptol = 500`.
Also `DEoptim` allows for parallel computing to save time.
Parallel computing may be employed via **parallel** or **foreach** package if we set `ParallelType = 1` or `ParallelType = 2`, respectively.
By default, `ParallelType = 1`.
By specifying `trace`, `DEoptim` prints progress of optimization at every `trace` iteration.
By default, `trace = 100`.


## Returned values of `LATEerror`

`LATEerror` returns a list that contains the following two lists.

* `pilot`: list containing the following elements for the pilot estimation based on `weight` as the weighting matrix
    + `estimate`: vector of parameter estimates
    + `se`: vector of standard errors
    + `ci`: matrix containing 95% confidence intervals for the parameters computed based on the asymptotic normality of the GMM estimator
    + `weight`: weighting matrix
    + `optim`: list returned by `DEoptim` that contains optimization results by DE algorithm

* `optimal`: list that contains the following elements for the two-step optimal estimation based on the optimal weighting matrix
    + `estimate`: vector of parameter estimates
    + `se`: vector of standard errors
    + `ci`: matrix containing 95% confidence intervals for the parameters computed based on the asymptotic normality of the GMM estimator
    + `overidentification`: vector for test statistic value and p-value for the over-identification test. It is `NULL` if there is no over-identification restriction
    + `weight`: weighting matrix
    + `optim`: list returned by `DEoptim` that contains optimization results by DE algorithm


## Remarks

We note the following general remarks.

* The exogenous variable $V$ must take $K$ consecutive discrete values with positive probabilities $\Pr(Z=z, V=v) > 0$ for $z=0,1$ and $v=1,2,\dots,K$. 

* The variables `Y`, `D`, `Z`, and `V` cannot contain missing values.

* DE optimization is based on random sampling numbers.
For reproducibility, it is strongly recommended that `set.seed` is called before implementing `LATEerror`.

* The performance of DE optimization may depend on the control arguments in `control`.
It is highly recommended to attempt several values for the arguments in `control` to find the global minimum.

* If an estimate for a parameter is equal to the specified lower or upper bound, it might imply a failure of optimization convergence.
In this case, you should try different values for the arguments in `control`.


## Example 1: GMM estimation via `LATEerror`

For the examples below, we generate the following simulated data.

```{r, eval = FALSE}
library(LATEerror)
library(dplyr)

set.seed(102)

n <- 1000
mydata <- data.frame(U1 = rnorm(n, 0, 1), U2 = rnorm(n, 0, 1))
mydata <- mydata %>%
  dplyr::mutate(Z = rbinom(n, size = 1, prob = 0.5)) %>%
  dplyr::mutate(V = rbinom(n, size = 1, prob = 0.5) + 1) %>%
  dplyr::mutate(Ds = ifelse(( -1.25 + Z + 0.5 * V - U1 >= 0), 1, 0)) %>%
  dplyr::mutate(D = Ds * rbinom(n, size = 1, prob = 0.8) + 
                  (1 - Ds) * rbinom(n, size = 1, prob = 0.2)) %>%
  dplyr::mutate(Y = 1 + Ds + U2)
```

Then we can implement the GMM estimation via `LATEerror`. 

```{r, eval = FALSE}
example1 <- LATEerror(Y = mydata$Y, D = mydata$D, Z = mydata$Z, V = mydata$V, 
                      weight = NULL, optimal = FALSE, lower = NULL, upper = NULL, 
                      control = list(strategy = 2, itermax = 10000, CR = 0.5, 
                                     F = 0.8, c = 0.05, parallelType = 1, 
                                     trace = FALSE))
```

We can see the results of the GMM estimation by following scripts.

```{r, eval = FALSE}
# estimates
example1$pilot$estimate

# standard errors
example1$pilot$se

# confidence intervals
example1$pilot$ci
```

## Example 2: GMM estimation manually

Without using `LATEerror`, we can also implement the GMM estimation manually.
To this end, we must introduce the weighting matrix for the GMM estimation and the lower and upper bounds of the parameters in the optimization manually.
We must also arrange the data by using `addindicators` in this package.
These procedures can be conducted as follows.

```{r, eval = FALSE}
# bounds for optimization
bound <- max(mydata$Y) - min(mydata$Y)
lower <- c(-bound, 0.001, 0.001, -bound, -bound, rep(0.001, 2), rep(0.001, 4))
upper <- c( bound, 0.999, 0.999,  bound,  bound, rep(0.499, 2), rep(0.999, 4))

# data
data <- addindicators(Y = mydata$Y, D = mydata$D, Z = mydata$Z, V = mydata$V)

# weighting matrix
weight <- diag(11)
```

Then we can implement the optimization manually.
For example, we can use L-BFGS-B method in `optim` as follows.

```{r, eval = FALSE}
# initial values for the optimization
x0 <- rep(0.2, 11)

# estimation
example2opt <- optim(par = x0, fn = criterion, 
                     method = "L-BFGS-B", lower = lower, upper = upper, 
                     data = data, weight = weight, equal = TRUE)
```

We note that it fails to optimize the function so that an estimate is equal to the lower bound of the parameter value.
This is because the performance of gradient-based local optimization may heavily depend on the initial value as discussed above.

To compute standard errors and confidence intervals, we can use `inference` function in this package.

```{r, eval = FALSE}
example2 <- inference(par = example2opt$par, 
                      data = data, weight = weight, equal = TRUE) 
```

In this case, we can check the results of the GMM estimation by the following scripts.

```{r, eval = FALSE}
# estimates
example2$estimate

# standard errors
example2$se

# confidence intervals
example2$ci
```


## See also
[DEoptim](https://cran.r-project.org/web/packages/DEoptim/index.html)


## References

Katharine Mullen, David Ardia, David Gil, Donald Windover, James Cline (2011): ["`DEoptim`: An R Package for Global Optimization by Differential Evolution"](http://www.jstatsoft.org/v40/i06/). Journal of Statistical Software, 40(6), 1-26.

Takahide Yanagi (2017): ["Inference on Local Average Treatment Effects for Misclassified Treatment""](http://dx.doi.org/10.2139/ssrn.3065923)
